---
title: 'STAT 443 Consulting Project Data Analysis'
author: "W. Jonas Reger"
date: "03/09/2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, eval=F)
```

```{r, eval=T}
set.seed(1)
options(warn = -1)
suppressMessages(library(tidyverse))
suppressMessages(library(modelr))
suppressMessages(library(caret))
suppressMessages(library(broom))
suppressMessages(library(leaps))
suppressMessages(library(MASS))
library(lmtest)
library(faraway)
library(glmnet)
```

## Data

Variables:

1. Ambient temperature (AT) C
2. Ambient pressure (AP) mbar
3. Ambient humidity (AH) (%)
4. Air filter difference pressure (AFDP) mbar
5. Gas turbine exhaust pressure (GTEP) mbar
6. Turbine inlet temperature (TIT) C
7. Turbine after temperature (TAT) C
8. Compressor discharge pressure (CDP) mbar
9. Turbine energy yield (TEY) MWH
10. Carbon monoxide (CO) mg/m3
11. Nitrogen oxides (NOX) mg/m3

```{r functions, eval=TRUE}
# Returns size of df
size = function(df) return(c(nrow(df), ncol(df)))

# Returns rmse
calc_rmse = function(actual, predicted){
  rmse = sqrt(mean((actual - predicted)^2))
  return(rmse)
}

# Returns validation rmse
gt_vali = function(model){
  rmse_val = calc_rmse(
    gt_val$CO,
    predict(update(model, data=gt_est), gt_val)
  )
  return(rmse_val)
}

gtr_vali = function(model){
  rmse_val = calc_rmse(
    gtr_val$CO,
    predict(update(model, data=gtr_est), gtr_val)
  )
  return(rmse_val)
}

gt_H_vali = function(model){
  rmse_val = calc_rmse(
    gt_H_val$CO,
    predict(update(model, data=gt_H_est), gt_H_val)
  )
  return(rmse_val)
}

gt_N_vali = function(model){
  rmse_val = calc_rmse(
    gt_N_val$CO,
    predict(update(model, data=gt_N_est), gt_N_val)
  )
  return(rmse_val)
}

mod.diag = function(mod, title){
  par(mfrow = c(1, 2))
  plot(fitted(mod), resid(mod), col="#13294b", pch=20,
     xlab = "Fitted", ylab = "Residuals", main = title)
  abline(h = 0, col = "#e84a27", lwd = 2)
  
  qqnorm(resid(mod), main = title, 
         col = "#13294b", pch=20)
  qqline(resid(mod), col = "#e84a27", lwd = 2)
  
  print(bptest(mod))
  rln = length(resid(mod))
  if(rln > 2 & rln < 5001){
    print(shapiro.test(resid(mod)))
  } else {
    print("sample size too large for shapiro.test()")
  }
  print("VIFs:")
  print(vif(mod))
  print(max(vif(mod)))
  print(paste("Adj. R^2:", round(summary(mod)$adj.r.squared, 4)))
}

gt_bic_sel = function(df, full){
  all_mod = summary(regsubsets(CO ~ ., data=df, nvmax=11, method='exhaustive'))
  p = length(coef(full))
  n = length(resid(full))
  mod_bic = n*log(all_mod$rss/n) + log(n)*(2:p)
  m = which.min(mod_bic)
  all_mod$which[m,]
}

adjr2 = function(mod) return(summary(mod)$adj.r.squared)

mod.qual = function(adjr2, vali, title){
  print(title)
  print("Adj. R^2:")
  print(adjr2)
  print("Val. RMSE:")
  print(vali)
  print(paste("Adj. R^2 Selection:", which.max(adjr2)))
  print(paste("Val. RMSE Selection:", which.min(vali)))
}
```

```{r, eval=T}
# summary(full)

# glance(full) %>%
  # dplyr::select(adj.r.squared, sigma, AIC, BIC, p.value)

# predictions <- full %>% predict(gt)
# data.frame(
#   R2 = R2(predictions, gt$CO),
#   RMSE = RMSE(predictions, gt$CO),
#   MAE = MAE(predictions, gt$CO)
# )
```

## Data Processing

```{r data, eval=T}
data = read.csv('gt_2012.csv', sep = ',')
head(data)

TEY_RANGE = ifelse(data$TEY >= 160, 'High', 
                   ifelse(data$TEY > 136, 'Above normal', 
                          ifelse(data$TEY >= 130, 'Normal', 
                                 ifelse(data$TEY > 110, 'Below normal', 'Low'))))
```

```{r gt, eval=T}
# Load packages
suppressMessages(library(tibble))
set.seed(443)

# gt data as tibble
gt = as_tibble(data)

# test-train split
gt_trn_idx = sample(nrow(gt), size = 0.8*nrow(gt))
gt_trn = gt[gt_trn_idx, ]
gt_tst = gt[-gt_trn_idx, ]

# estimation-validation split
gt_est_idx = sample(nrow(gt_trn), size = 0.8*nrow(gt_trn))
gt_est = gt_trn[gt_est_idx, ]
gt_val = gt_trn[-gt_est_idx, ]

# check data
# head(gte_trn)
size(gt)
```

```{r gtr, eval=T}
# gtr data
gtr = cbind(gt, TEY_RANGE)

# test-train split
gtr_trn_idx = sample(nrow(gtr), size = 0.8*nrow(gtr))
gtr_trn = gtr[gtr_trn_idx, ]
gtr_tst = gtr[-gtr_trn_idx, ]

# estimation-validation split
gtr_est_idx = sample(nrow(gtr_trn), size = 0.8*nrow(gtr_trn))
gtr_est = gtr_trn[gtr_est_idx, ]
gtr_val = gtr_trn[-gtr_est_idx, ]

# check data
# head(gtr_trn)
size(gtr)
```

```{r gt_H, eval=T}
# gt_H filter
gt_H = filter(gt, TEY >= 160)

# test-train split
gt_H_trn_idx = sample(nrow(gt_H), size = 0.8*nrow(gt_H))
gt_H_trn = gt_H[gt_H_trn_idx, ]
gt_H_tst = gt_H[-gt_H_trn_idx, ]

# estimation-validation split
gt_H_est_idx = sample(nrow(gt_H_trn), size = 0.8*nrow(gt_H_trn))
gt_H_est = gt_H_trn[gt_H_est_idx, ]
gt_H_val = gt_H_trn[-gt_H_est_idx, ]

# check data
# head(gt_H_trn)
size(gt_H)
```

```{r gt_N, eval=T}
# gt_N filter
gt_N = filter(gt, TEY >= 130 | TEY <= 136)

# test-train split
gt_N_trn_idx = sample(nrow(gt_N), size = 0.8*nrow(gt_N))
gt_N_trn = gt_N[gt_N_trn_idx, ]
gt_N_tst = gt_N[-gt_N_trn_idx, ]

# estimation-validation split
gt_N_est_idx = sample(nrow(gt_N_trn), size = 0.8*nrow(gt_N_trn))
gt_N_est = gt_N_trn[gt_N_est_idx, ]
gt_N_val = gt_N_trn[-gt_N_est_idx, ]

# check data
# head(gt_N_trn)
size(gt_N)
```


## Data Summary

```{r, eval=T}
# summary(gte)
# table(gte$TEY_RANGE)
```

## Plots

```{r pairplots and correlations}
pairs(gt, col="#13294b", main="Full Data Pair Plot")
pairs(gt_H, col="#13294b", main="High Yield Data Pair Plot")
pairs(gt_N, col="#13294b", main="Normal Yield Data Pair Plot")

round(cor(gt), 2)
round(cor(gt_H), 2)
round(cor(gt_N), 2)
```

## Model Building & Diagnostics

```{r int and full models, eval=TRUE}
# intmod = lm(CO ~ 1, data = gt)
full = lm(CO ~ ., data = gt)
full_R = lm(CO ~ ., data = gtr)
full_H = update(full, data = gt_H)
full_N = update(full, data = gt_N)
```

```{r Full model diagnostics, eval=T}
mod.diag(full, "Full Data from Full Model")
mod.diag(full_R, "Full Data from Full Range Model")
mod.diag(full_H, "High Yield Data from Full Model")
mod.diag(full_N, "Normal Yield Data from Full Model")

```

```{r, eval=TRUE}
fit = update(full, .~.-TIT-GTEP-CDP)
mod.diag(fit, "Fit, Full Data")

fit_R = update(full_R, .~.-TIT-GTEP-CDP-TEY)
mod.diag(fit_R, "Fit, Full Range Data")

fit_H = update(full_H, .~.-TAT-TEY-CDP)
mod.diag(fit_H, "Fit, High Yield Data")

fit_H2 = update(full_H, .~.-TAT-CDP)
mod.diag(fit_H2, "Fit 2, High Yield Data")

# Same as fit model
fit_N = update(full_N, .~.-TIT-GTEP-CDP)
mod.diag(fit_N, "Fit, Normal Yield Data")
```

## Variable Selection

```{r, eval=TRUE}
gt_bic_sel(gt, full)
gt_bic_sel(gtr, full)
gt_bic_sel(gt_H, full)
gt_bic_sel(gt_N, full)

bic_mod = update(full, .~.-AP-AFDP-TEY-CDP)
bic_mod2 = update(full, .~.-AFDP-CDP)

bic_mod_R = update(full_R, .~.-AP)

bic_mod_H = update(full, .~.-AP-AH-AFDP, data=gt_H)
bic_mod_H2 = update(full, .~.-AFDP, data=gt_H)

# Same as bic_mod and bic_mod2
bic_mod_N = update(full, .~.-AP-AFDP-TEY-CDP, data=gt_N)
bic_mod_N2 = update(full, .~.-AFDP-CDP, data=gt_N)
```

```{r, eval=T}
# Adjusted R squared
# Full data models
adjr2_full = c(adjr2(full), adjr2(fit), adjr2(bic_mod), adjr2(bic_mod2))
# Full Range data models
adjr2_R = c(adjr2(full_R), adjr2(fit_R), adjr2(bic_mod_R))
# High Yield data models
adjr2_H = c(adjr2(full_H), adjr2(fit_H), adjr2(fit_H2), adjr2(bic_mod_H), adjr2(bic_mod_H2))
# Normal Yield data models
adjr2_N = c(adjr2(full_N), adjr2(fit_N), adjr2(bic_mod_N), adjr2(bic_mod_N2))

# Validation RMSE
# Full data models
vali_full = c(gt_vali(full), gt_vali(fit), gt_vali(bic_mod), gt_vali(bic_mod2))
# Full Range data models
vali_R = c(gtr_vali(full_R), gtr_vali(fit_R), gtr_vali(bic_mod_R))
# High Yield data models
vali_H = c(gt_H_vali(full_H), gt_H_vali(fit_H), gt_H_vali(fit_H2), gt_H_vali(bic_mod_H), gt_H_vali(bic_mod_H2))
# Normal Yield data models
vali_N = c(gt_N_vali(full_N), gt_N_vali(fit_N), gt_N_vali(bic_mod_N), gt_N_vali(bic_mod_N2))

# Model Selections
mod.qual(adjr2_full, vali_full, "Full Data Models")
mod.qual(adjr2_R, vali_R, "Full Range Data Models")
mod.qual(adjr2_H, vali_H, "High Yield Data Models")
mod.qual(adjr2_N, vali_N, "Normal Yield Data Models")
```

* (Full Data) The `full` model has the highest Adjusted $R^2$ and lowest validation RMSE.
* (Full Range Data) The `full_R` model has the highest Adjusted $R^2$ and the `bic_mod_R` model has the lowest validation RMSE.
* (High Yield Data) The `bic_mod_H` model (Full Data) has the highest Adjusted $R^2$ and lowest validation RMSE.
* (Normal Yield Data) The `full_N` model has the highest Adjusted $R^2$ and the `bic_mod_N` model has the lowest validation RMSE.

## Etc

```{r}
f.full = summary(full)$fstat[1]
f.full
fstats <- numeric(1000)
for (i in 1:1000){
  temp.mod = update(full, sample(.)~.)
  fstats[i] = summary(temp.mod)$fstat[1]

}
mean(fstats > f.full)
hist(fstats, breaks=100)
```

```{r}
lambdas = 10^seq(2, -4, by = -.1)
X = model.matrix(update(full, .~.-1))
Y = gt$CO

fit.ridge = glmnet(X, Y, family="gaussian", alpha = 0, lambda = lambdas)
# plot(fit.ridge)

cv.fit = cv.glmnet(X, Y, family="gaussian", alpha = 0, lambda = lambdas)
plot(cv.fit)
coef(cv.fit, s = "lambda.min")

# opt_lambda <- cv.fit$lambda.min
# opt_lambda

plot(fit.ridge, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:ncol(X), legend = colnames(X), cex = .7)
```

```{r R.L.EN, eval=TRUE}
# set.seed(443)
# X.trn = data.matrix(subset(gt_trn, select=-10))
# Y.trn = gt_trn$CO
# 
# X.tst = data.matrix(subset(gt_tst, select=-10))
# Y.tst = gt_tst$CO
# 
# fit.lasso = glmnet(X.trn, Y.trn, family="gaussian", alpha=1)
# fit.ridge = glmnet(X.trn, Y.trn, family="gaussian", alpha=0)
# fit.elnet = glmnet(X.trn, Y.trn, family="gaussian", alpha=.5)

# par(mfrow=c(2,2))
# plot(fit.lasso, xvar="lambda", main = "LASSO")
# legend("bottomright", lwd = 1, col = 1:ncol(X.trn), legend = colnames(X.trn), cex = .7)
# plot(fit.ridge, xvar="lambda", main = "Ridge")
# legend("bottomright", lwd = 1, col = 1:ncol(X.trn), legend = colnames(X.trn), cex = .7)
# plot(fit.elnet, xvar="lambda", main = "Elastic Net")
# legend("bottomright", lwd = 1, col = 1:ncol(X.trn), legend = colnames(X.trn), cex = .7)
# 
# for (i in 0:10) {
#   assign(paste("fit", i, sep=""), 
#          cv.glmnet(X.trn, Y.trn, type.measure="mse", alpha=i/10,family="gaussian"))
# }
# par(mfrow=c(3,2))
# plot(fit0, main="Ridge")
# plot(fit2, main="Elastic Net")
# plot(fit4, main="Elastic Net")
# plot(fit6, main="Elastic Net")
# plot(fit8, main="Elastic Net")
# plot(fit10, main="LASSO")

# coef(fit0, s = "lambda.1se")
# coef(fit10, s = "lambda.1se")
# coef(fit0, s = "lambda.min")
# coef(fit10, s = "lambda.min")

# yhat0 <- predict(fit0, s=fit0$lambda.1se, newx=X.tst)
# yhat1 <- predict(fit1, s=fit1$lambda.1se, newx=X.tst)
# yhat2 <- predict(fit2, s=fit2$lambda.1se, newx=X.tst)
# yhat3 <- predict(fit3, s=fit3$lambda.1se, newx=X.tst)
# yhat4 <- predict(fit4, s=fit4$lambda.1se, newx=X.tst)
# yhat5 <- predict(fit5, s=fit5$lambda.1se, newx=X.tst)
# yhat6 <- predict(fit6, s=fit6$lambda.1se, newx=X.tst)
# yhat7 <- predict(fit7, s=fit7$lambda.1se, newx=X.tst)
# yhat8 <- predict(fit8, s=fit8$lambda.1se, newx=X.tst)
# yhat9 <- predict(fit9, s=fit9$lambda.1se, newx=X.tst)
# yhat10 <- predict(fit10, s=fit10$lambda.1se, newx=X.tst)
# 
# mse0 <- mean((Y.tst - yhat0)^2)
# mse1 <- mean((Y.tst - yhat1)^2)
# mse2 <- mean((Y.tst - yhat2)^2)
# mse3 <- mean((Y.tst - yhat3)^2)
# mse4 <- mean((Y.tst - yhat4)^2)
# mse5 <- mean((Y.tst - yhat5)^2)
# mse6 <- mean((Y.tst - yhat6)^2)
# mse7 <- mean((Y.tst - yhat7)^2)
# mse8 <- mean((Y.tst - yhat8)^2)
# mse9 <- mean((Y.tst - yhat9)^2)
# mse10 <- mean((Y.tst - yhat10)^2)

# MSE = c(mse0, mse1, mse2, mse3, mse4, mse5, mse6, mse7, mse8, mse9, mse10)
# par(mfrow=c(1,1))
# plot(0:10, MSE)
# which.min(MSE)
# coef(fit6, s = "lambda.1se")
# coef(fit6, s = "lambda.min")

# fit.lasso.cv <- cv.glmnet(X.trn, Y.trn, type.measure="mse", alpha=1, 
#                           family="gaussian")
# fit.ridge.cv <- cv.glmnet(X.trn, Y.trn, type.measure="mse", alpha=0,
#                           family="gaussian")
# fit.elnet.cv <- cv.glmnet(X.trn, Y.trn, type.measure="mse", alpha=.5,
#                           family="gaussian")

# for (i in 0:10) {
#      assign(paste("fit", i, sep=""), cv.glmnet(X.trn, Y.trn, type.measure="mse", 
#                                                alpha=i/10,family="gaussian"))
# }
# par(mfrow=c(3,2))
# plot(fit0, main="Ridge")
# plot(fit2, main="Elastic Net")
# plot(fit4, main="Elastic Net")
# plot(fit6, main="Elastic Net")
# plot(fit8, main="Elastic Net")
# plot(fit10, main="LASSO")

# coef(fit0, s = "lambda.1se")
# coef(fit10, s = "lambda.1se")
# coef(fit0, s = "lambda.min")
# coef(fit10, s = "lambda.min")
# 
# yhat0 <- predict(fit0, s=fit0$lambda.1se, newx=X.tst)
# yhat1 <- predict(fit1, s=fit1$lambda.1se, newx=X.tst)
# yhat2 <- predict(fit2, s=fit2$lambda.1se, newx=X.tst)
# yhat3 <- predict(fit3, s=fit3$lambda.1se, newx=X.tst)
# yhat4 <- predict(fit4, s=fit4$lambda.1se, newx=X.tst)
# yhat5 <- predict(fit5, s=fit5$lambda.1se, newx=X.tst)
# yhat6 <- predict(fit6, s=fit6$lambda.1se, newx=X.tst)
# yhat7 <- predict(fit7, s=fit7$lambda.1se, newx=X.tst)
# yhat8 <- predict(fit8, s=fit8$lambda.1se, newx=X.tst)
# yhat9 <- predict(fit9, s=fit9$lambda.1se, newx=X.tst)
# yhat10 <- predict(fit10, s=fit10$lambda.1se, newx=X.tst)

# mse0 <- mean((Y.tst - yhat0)^2)
# mse1 <- mean((Y.tst - yhat1)^2)
# mse2 <- mean((Y.tst - yhat2)^2)
# mse3 <- mean((Y.tst - yhat3)^2)
# mse4 <- mean((Y.tst - yhat4)^2)
# mse5 <- mean((Y.tst - yhat5)^2)
# mse6 <- mean((Y.tst - yhat6)^2)
# mse7 <- mean((Y.tst - yhat7)^2)
# mse8 <- mean((Y.tst - yhat8)^2)
# mse9 <- mean((Y.tst - yhat9)^2)
# mse10 <- mean((Y.tst - yhat10)^2)
# 
# MSE = c(mse0, mse1, mse2, mse3, mse4, mse5, mse6, mse7, mse8, mse9, mse10)
# par(mfrow=c(1,1))
# plot(0:10, MSE)
# which.min(MSE)
# coef(fit8, s = "lambda.1se")
# coef(fit8, s = "lambda.min")



RLEN = function(x.train, y.train, x.test, y.test){
  set.seed(443)
  X.trn = data.matrix(x.train)
  Y.trn = y.train
  X.tst = data.matrix(x.test)
  Y.tst = y.test
  
  for (i in 0:10) {
    assign(paste("fit", i, sep=""), 
           cv.glmnet(X.trn, Y.trn, type.measure="mse", alpha=i/10,family="gaussian"))
  }
  
  yhat0 <- predict(fit0, s=fit0$lambda.1se, newx=X.tst)
  yhat1 <- predict(fit1, s=fit1$lambda.1se, newx=X.tst)
  yhat2 <- predict(fit2, s=fit2$lambda.1se, newx=X.tst)
  yhat3 <- predict(fit3, s=fit3$lambda.1se, newx=X.tst)
  yhat4 <- predict(fit4, s=fit4$lambda.1se, newx=X.tst)
  yhat5 <- predict(fit5, s=fit5$lambda.1se, newx=X.tst)
  yhat6 <- predict(fit6, s=fit6$lambda.1se, newx=X.tst)
  yhat7 <- predict(fit7, s=fit7$lambda.1se, newx=X.tst)
  yhat8 <- predict(fit8, s=fit8$lambda.1se, newx=X.tst)
  yhat9 <- predict(fit9, s=fit9$lambda.1se, newx=X.tst)
  yhat10 <- predict(fit10, s=fit10$lambda.1se, newx=X.tst)
  
  mse0 <- mean((Y.tst - yhat0)^2)
  mse1 <- mean((Y.tst - yhat1)^2)
  mse2 <- mean((Y.tst - yhat2)^2)
  mse3 <- mean((Y.tst - yhat3)^2)
  mse4 <- mean((Y.tst - yhat4)^2)
  mse5 <- mean((Y.tst - yhat5)^2)
  mse6 <- mean((Y.tst - yhat6)^2)
  mse7 <- mean((Y.tst - yhat7)^2)
  mse8 <- mean((Y.tst - yhat8)^2)
  mse9 <- mean((Y.tst - yhat9)^2)
  mse10 <- mean((Y.tst - yhat10)^2)
  
  MSE = c(mse0, mse1, mse2, mse3, mse4, mse5, mse6, mse7, mse8, mse9, mse10)
  par(mfrow=c(1,1))
  plot(0:10, MSE)
  print(min(MSE))
  print(which.min(MSE))
  print(coef(fit0, s = "lambda.1se"))
  print(coef(fit1, s = "lambda.1se"))
  print(coef(fit2, s = "lambda.1se"))
  print(coef(fit3, s = "lambda.1se"))
  print(coef(fit4, s = "lambda.1se"))
  print(coef(fit5, s = "lambda.1se"))
  print(coef(fit6, s = "lambda.1se"))
  print(coef(fit7, s = "lambda.1se"))
  print(coef(fit8, s = "lambda.1se"))
  print(coef(fit9, s = "lambda.1se"))
  print(coef(fit10, s = "lambda.1se"))
}

# RLEN(subset(gt_trn, select=-10), gt_trn$CO,
#      subset(gt_tst, select=-10), gt_tst$CO)
# 
# RLEN(subset(gtr_trn, select=-10), gtr_trn$CO,
#      subset(gtr_tst, select=-10), gtr_tst$CO)
# 
# RLEN(subset(gt_H_trn, select=-10), gt_H_trn$CO,
#      subset(gt_H_tst, select=-10), gt_H_tst$CO)
# 
# RLEN(subset(gt_N_trn, select=-10), gt_N_trn$CO,
#      subset(gt_N_tst, select=-10), gt_N_tst$CO)

# head(gt[, -c(5, 6, 9)])
# head(gtr[, -c(5, 6, 8, 9)])
# head(gt_H[, -c(7:9)])
# head(gt_H[, -c(7, 9)])
# head(gt_N[, -c(5, 6, 9)])

# RLEN(subset(gt_trn, select= -c(5, 6, 9, 10)), gt_trn$CO,
#      subset(gt_tst, select= -c(5, 6, 9, 10)), gt_tst$CO)

# RLEN(subset(gtr_trn, select= -c(5, 6, 8, 9, 10)), gtr_trn$CO,
#      subset(gtr_tst, select= -c(5, 6, 8, 9, 10)), gtr_tst$CO)
```


## Potential Models

### Full Models

`full = lm(CO ~ ., data = gt)`
`full_R = lm(CO ~ ., data = gtr)`
`full_H = update(full, data = gt_H)`
`full_N = update(full, data = gt_N)`

### VIF Elimination

`fit = update(full, .~.-TIT-GTEP-CDP)`
`fit_R = update(full_R, .~.-TIT-GTEP-CDP-TEY)`
`fit_H = update(full_H, .~.-TAT-TEY-CDP)`
`fit_H2 = update(full_H, .~.-TAT-CDP)`
`fit_N = update(full_N, .~.-TIT-GTEP-CDP)` _(Same as fit model)_

### BIC Selection

`bic_mod = update(full, .~.-AP-AFDP-TEY-CDP)`
`bic_mod2 = update(full, .~.-AFDP-CDP)`
`bic_mod_R = update(full_R, .~.-AP)`
`bic_mod_H = update(full, .~.-AP-AH-AFDP, data=gt_H)`
`bic_mod_H2 = update(full, .~.-AFDP, data=gt_H)`
`bic_mod_N = update(full, .~.-AP-AFDP-TEY-CDP, data=gt_N)` _(Same as bic_mod model)_
`bic_mod_N2 = update(full, .~.-AFDP-CDP, data=gt_N)` _(Same as bic_mod2 model)_

### Adjusted R^2 & Validation RMSE

`full`, `full_R`, `bic_mod_R`, `bic_mod_H`, `full_N`, `bic_mod_N`






















